{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Spring 99 <img src = 'https://ece.ut.ac.ir/cict-theme/images/footer-logo.png' alt=\"Tehran-University-Logo\" width=\"150\" height=\"150\" align=\"right\">\n",
    "## Project 3 : Text Processing and Bayesian Networks\n",
    "### Dr. Hakimeh Fadaei\n",
    "### By Omid Vaheb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "In this project, At first, we inspected data and cleaned it and normalize it for maximum accuracy. The next step was to implement a Bayesian network with 2 classes and a bag of word method to calculate words probabilities on the clean dataset. Then I implemented the network on a classification with 3 classes and used the model to classify some unknown data and got a good accuracy by checking with the original dataset in Kaggle.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "In this project we are asked to classify news category based on it's sgort description and headline. The solution is to implement Naive Bayes which is a simple Bayesian Network having one node of class and some other nodes representing features that are only connected to class node meaning that they are independent from each other. Another method used in this project is bag of words which is considering probbility of word regardless of their position in sentence. It means that probability of word to appear in any position in sentence is consistent. Now let's tell remaining explanations by analyzing code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we import requiered libraries such as numpy and pandas for dataframe functions and re and nltk for data cleaning. Random and copy are used for minor parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Omid\n",
      "[nltk_data]     Vaheb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Omid\n",
      "[nltk_data]     Vaheb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Omid\n",
      "[nltk_data]     Vaheb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to laod dataset and inspect it a little. As you will see further it is dataset of news published on huffington post newspaper containing the author, date, headline, link and short description and the category of each news. It could be seen that there are some rows with no descriptions so I removed them due to project circumstances. The other thing that we can understand is that there are 3 type of news categories in the dataset: Business and Travel and Style and Beauty. Also the indexes in this project are important so they won't change throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Katherine LaGrave, ContributorTravel writer an...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>EccentriCities: Bingo Parties, Paella and Isla...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eccentric...</td>\n",
       "      <td>Påskekrim is merely the tip of the proverbial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ben Hallman</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2014-06-09</td>\n",
       "      <td>Lawyers Are Now The Driving Force Behind Mortg...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/mortgage-...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Jessica Misener</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Madonna 'Truth Or Dare' Shoe Line To Debut Thi...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/madonna-s...</td>\n",
       "      <td>Madonna is slinking her way into footwear now,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Victor and Mary, Contributor\\n2Sense-LA.com</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2013-12-17</td>\n",
       "      <td>Sophistication and Serenity on the Las Vegas S...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/las-vegas...</td>\n",
       "      <td>But what if you're a 30-something couple that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Emily Cohn, Contributor</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2015-03-19</td>\n",
       "      <td>It's Still Pretty Hard For Women To Get Free B...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/free-birt...</td>\n",
       "      <td>Obamacare was supposed to make birth control f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Jessica Misener</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Madonna 'Truth Or Dare' Shoe Line To Debut Thi...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/madonna-s...</td>\n",
       "      <td>Madonna previously released a Truth or Dare fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Ellie Krupnick</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2014-01-08</td>\n",
       "      <td>A Mean Girls Jewelry Line? That Is SO Fetch</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/mean-girl...</td>\n",
       "      <td>\"Most all of our favorite movies have been cap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Simone Kitchens</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2012-10-18</td>\n",
       "      <td>Beauty Street Style: Stephanie Vanessa, Stylis...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/beauty-st...</td>\n",
       "      <td>Take notes, ladies.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Ellie Krupnick</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>2013-01-31</td>\n",
       "      <td>Isabel Lucas Attempts Christian Dior Runway Lo...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/isabel-lu...</td>\n",
       "      <td>Other times, something is lost in translation....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Budget Travel, Contributor\\nYour source for va...</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>2013-09-11</td>\n",
       "      <td>Warning: You'll Never Get These Items Through ...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/warning-y...</td>\n",
       "      <td>There are a few no-no's that U.S. customs will...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            authors        category  \\\n",
       "0      0  Katherine LaGrave, ContributorTravel writer an...          TRAVEL   \n",
       "1      1                                        Ben Hallman        BUSINESS   \n",
       "2      2                                    Jessica Misener  STYLE & BEAUTY   \n",
       "3      3        Victor and Mary, Contributor\\n2Sense-LA.com          TRAVEL   \n",
       "4      4                            Emily Cohn, Contributor        BUSINESS   \n",
       "5      5                                    Jessica Misener  STYLE & BEAUTY   \n",
       "6      6                                     Ellie Krupnick  STYLE & BEAUTY   \n",
       "7      7                                    Simone Kitchens  STYLE & BEAUTY   \n",
       "8      8                                     Ellie Krupnick  STYLE & BEAUTY   \n",
       "9      9  Budget Travel, Contributor\\nYour source for va...          TRAVEL   \n",
       "\n",
       "         date                                           headline  \\\n",
       "0  2014-05-07  EccentriCities: Bingo Parties, Paella and Isla...   \n",
       "1  2014-06-09  Lawyers Are Now The Driving Force Behind Mortg...   \n",
       "2  2012-03-12  Madonna 'Truth Or Dare' Shoe Line To Debut Thi...   \n",
       "3  2013-12-17  Sophistication and Serenity on the Las Vegas S...   \n",
       "4  2015-03-19  It's Still Pretty Hard For Women To Get Free B...   \n",
       "5  2012-03-12  Madonna 'Truth Or Dare' Shoe Line To Debut Thi...   \n",
       "6  2014-01-08        A Mean Girls Jewelry Line? That Is SO Fetch   \n",
       "7  2012-10-18  Beauty Street Style: Stephanie Vanessa, Stylis...   \n",
       "8  2013-01-31  Isabel Lucas Attempts Christian Dior Runway Lo...   \n",
       "9  2013-09-11  Warning: You'll Never Get These Items Through ...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/eccentric...   \n",
       "1  https://www.huffingtonpost.com/entry/mortgage-...   \n",
       "2  https://www.huffingtonpost.com/entry/madonna-s...   \n",
       "3  https://www.huffingtonpost.com/entry/las-vegas...   \n",
       "4  https://www.huffingtonpost.com/entry/free-birt...   \n",
       "5  https://www.huffingtonpost.com/entry/madonna-s...   \n",
       "6  https://www.huffingtonpost.com/entry/mean-girl...   \n",
       "7  https://www.huffingtonpost.com/entry/beauty-st...   \n",
       "8  https://www.huffingtonpost.com/entry/isabel-lu...   \n",
       "9  https://www.huffingtonpost.com/entry/warning-y...   \n",
       "\n",
       "                                   short_description  \n",
       "0  Påskekrim is merely the tip of the proverbial ...  \n",
       "1                                                NaN  \n",
       "2  Madonna is slinking her way into footwear now,...  \n",
       "3  But what if you're a 30-something couple that ...  \n",
       "4  Obamacare was supposed to make birth control f...  \n",
       "5  Madonna previously released a Truth or Dare fr...  \n",
       "6  \"Most all of our favorite movies have been cap...  \n",
       "7                                Take notes, ladies.  \n",
       "8  Other times, something is lost in translation....  \n",
       "9  There are a few no-no's that U.S. customs will...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22925 entries, 0 to 22924\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   index              22925 non-null  int64 \n",
      " 1   authors            18523 non-null  object\n",
      " 2   category           22925 non-null  object\n",
      " 3   date               22925 non-null  object\n",
      " 4   headline           22924 non-null  object\n",
      " 5   link               22925 non-null  object\n",
      " 6   short_description  21703 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                   0\n",
       "authors              4402\n",
       "category                0\n",
       "date                    0\n",
       "headline                1\n",
       "link                    0\n",
       "short_description    1222\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data['short_description'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                   0\n",
       "authors              4182\n",
       "category                0\n",
       "date                    0\n",
       "headline                0\n",
       "link                    0\n",
       "short_description       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I said above there were 1222 rows without short description that i had removed them and filtered dataset keeping only index, category, headline and short description columns because the other ones weren't helpful for us. Using only short description would have resulted an acceptable answer but adding headline increased accuracy up to 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = data.loc[:, ['index', 'category', 'headline', 'short_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>EccentriCities: Bingo Parties, Paella and Isla...</td>\n",
       "      <td>Påskekrim is merely the tip of the proverbial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>Madonna 'Truth Or Dare' Shoe Line To Debut Thi...</td>\n",
       "      <td>Madonna is slinking her way into footwear now,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>Sophistication and Serenity on the Las Vegas S...</td>\n",
       "      <td>But what if you're a 30-something couple that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>It's Still Pretty Hard For Women To Get Free B...</td>\n",
       "      <td>Obamacare was supposed to make birth control f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>Madonna 'Truth Or Dare' Shoe Line To Debut Thi...</td>\n",
       "      <td>Madonna previously released a Truth or Dare fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22920</th>\n",
       "      <td>22920</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>Rihanna's Sweatpants Top Our Worst-Dressed Lis...</td>\n",
       "      <td>The theme for this week's worst-dressed list i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22921</th>\n",
       "      <td>22921</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>Gisele Gives Birth To Baby Girl: Model Welcome...</td>\n",
       "      <td>Gisele finally confirms she's pregnant... by h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22922</th>\n",
       "      <td>22922</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>15 Golden Giveaways</td>\n",
       "      <td>Travelzoo, a global Internet media company, is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22923</th>\n",
       "      <td>22923</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>Talking With Actor, Activist And Innkeeper Ric...</td>\n",
       "      <td>\"We wanted to save it, so we bought it with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22924</th>\n",
       "      <td>22924</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>7 DIY Revitalizing Beauty Tips for Hair and Skin</td>\n",
       "      <td>Here are some simple head-to-toe DIY preventat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21703 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index        category  \\\n",
       "0          0          TRAVEL   \n",
       "2          2  STYLE & BEAUTY   \n",
       "3          3          TRAVEL   \n",
       "4          4        BUSINESS   \n",
       "5          5  STYLE & BEAUTY   \n",
       "...      ...             ...   \n",
       "22920  22920  STYLE & BEAUTY   \n",
       "22921  22921  STYLE & BEAUTY   \n",
       "22922  22922          TRAVEL   \n",
       "22923  22923          TRAVEL   \n",
       "22924  22924  STYLE & BEAUTY   \n",
       "\n",
       "                                                headline  \\\n",
       "0      EccentriCities: Bingo Parties, Paella and Isla...   \n",
       "2      Madonna 'Truth Or Dare' Shoe Line To Debut Thi...   \n",
       "3      Sophistication and Serenity on the Las Vegas S...   \n",
       "4      It's Still Pretty Hard For Women To Get Free B...   \n",
       "5      Madonna 'Truth Or Dare' Shoe Line To Debut Thi...   \n",
       "...                                                  ...   \n",
       "22920  Rihanna's Sweatpants Top Our Worst-Dressed Lis...   \n",
       "22921  Gisele Gives Birth To Baby Girl: Model Welcome...   \n",
       "22922                                15 Golden Giveaways   \n",
       "22923  Talking With Actor, Activist And Innkeeper Ric...   \n",
       "22924   7 DIY Revitalizing Beauty Tips for Hair and Skin   \n",
       "\n",
       "                                       short_description  \n",
       "0      Påskekrim is merely the tip of the proverbial ...  \n",
       "2      Madonna is slinking her way into footwear now,...  \n",
       "3      But what if you're a 30-something couple that ...  \n",
       "4      Obamacare was supposed to make birth control f...  \n",
       "5      Madonna previously released a Truth or Dare fr...  \n",
       "...                                                  ...  \n",
       "22920  The theme for this week's worst-dressed list i...  \n",
       "22921  Gisele finally confirms she's pregnant... by h...  \n",
       "22922  Travelzoo, a global Internet media company, is...  \n",
       "22923  \"We wanted to save it, so we bought it with th...  \n",
       "22924  Here are some simple head-to-toe DIY preventat...  \n",
       "\n",
       "[21703 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is cleaning data which i had written a function called cleanData for this purpose. It combines headline and short description each row and put it in text column and then lowercase all of the letters in the text and spliting them by non alphabetic characters. The next step was to convert word to their root format using nlkt librry methods stemming and lemmatizating. I used both of them nd got almost same answers (stemming was better a little bit so I used it). All of these actions are necessary in order t get an accurate answer. If we didn't lowercased all of our words there would be many versions of a word  with same value for us and in the state of probability calculation it will make our accuracy worse. A simple example for this would be the word money and Money which need to have same probability for business class but seperating them would split their probabilities and will lower the probability of this world. Another part of cleaning is converting word to their root. We could use stemming and lemmatizating. Stemming will only remove prefix and postixes, ing's , s's and things like this but won't change the format of verbs to their root like lemmatizating. In general it is a weaker method but it works better here. Another part of the data cleaning is removing word that are used alot in texts such as this, that, the and ... and won't help us in our classification method. I checked esults without lowercasing and root finding and each one decreased accuracy at least 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "def cleanData(row):\n",
    "    row['text'] = row['headline'] + ' ' + row['short_description']\n",
    "    row['text'] = row['text'].lower()\n",
    "    wordTokens = re.split('\\W', row['text'])\n",
    "    filtered = [] \n",
    "    ps = PorterStemmer()\n",
    "    for word in wordTokens:\n",
    "        if word not in stopWords and not word == '' and not word.isdigit():\n",
    "            root = ps.stem(word)\n",
    "            filtered.append(root)\n",
    "    row['text'] = filtered\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.apply(cleanData, axis = 'columns')\n",
    "dataframe = dataframe.loc[:, ['index', 'category', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[eccentr, bingo, parti, paella, island, hop, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>[madonna, truth, dare, shoe, line, debut, fall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[sophist, seren, la, vega, strip, someth, coup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[still, pretti, hard, women, get, free, birth,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>[madonna, truth, dare, shoe, line, debut, fall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22920</th>\n",
       "      <td>22920</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>[rihanna, sweatpant, top, worst, dress, list, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22921</th>\n",
       "      <td>22921</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>[gisel, give, birth, babi, girl, model, welcom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22922</th>\n",
       "      <td>22922</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[golden, giveaway, travelzoo, global, internet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22923</th>\n",
       "      <td>22923</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[talk, actor, activist, innkeep, richard, gere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22924</th>\n",
       "      <td>22924</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>[diy, revit, beauti, tip, hair, skin, simpl, h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21703 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index        category  \\\n",
       "0          0          TRAVEL   \n",
       "2          2  STYLE & BEAUTY   \n",
       "3          3          TRAVEL   \n",
       "4          4        BUSINESS   \n",
       "5          5  STYLE & BEAUTY   \n",
       "...      ...             ...   \n",
       "22920  22920  STYLE & BEAUTY   \n",
       "22921  22921  STYLE & BEAUTY   \n",
       "22922  22922          TRAVEL   \n",
       "22923  22923          TRAVEL   \n",
       "22924  22924  STYLE & BEAUTY   \n",
       "\n",
       "                                                    text  \n",
       "0      [eccentr, bingo, parti, paella, island, hop, o...  \n",
       "2      [madonna, truth, dare, shoe, line, debut, fall...  \n",
       "3      [sophist, seren, la, vega, strip, someth, coup...  \n",
       "4      [still, pretti, hard, women, get, free, birth,...  \n",
       "5      [madonna, truth, dare, shoe, line, debut, fall...  \n",
       "...                                                  ...  \n",
       "22920  [rihanna, sweatpant, top, worst, dress, list, ...  \n",
       "22921  [gisel, give, birth, babi, girl, model, welcom...  \n",
       "22922  [golden, giveaway, travelzoo, global, internet...  \n",
       "22923  [talk, actor, activist, innkeep, richard, gere...  \n",
       "22924  [diy, revit, beauti, tip, hair, skin, simpl, h...  \n",
       "\n",
       "[21703 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with 2 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeTB = dataframe.loc[dataframe['category'] != 'STYLE & BEAUTY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[eccentr, bingo, parti, paella, island, hop, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[sophist, seren, la, vega, strip, someth, coup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[still, pretti, hard, women, get, free, birth,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[warn, never, get, item, custom, u, custom, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[get, away, without, get, airplan, trip, easi,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22898</th>\n",
       "      <td>22898</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[berlin, cruis, ship, northern, europ, scandin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22910</th>\n",
       "      <td>22910</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[peac, corp, taught, manag, peopl, simultan, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22914</th>\n",
       "      <td>22914</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[bank, america, appear, flip, firearm, promis,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22922</th>\n",
       "      <td>22922</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[golden, giveaway, travelzoo, global, internet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22923</th>\n",
       "      <td>22923</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[talk, actor, activist, innkeep, richard, gere...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13029 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  category                                               text\n",
       "0          0    TRAVEL  [eccentr, bingo, parti, paella, island, hop, o...\n",
       "3          3    TRAVEL  [sophist, seren, la, vega, strip, someth, coup...\n",
       "4          4  BUSINESS  [still, pretti, hard, women, get, free, birth,...\n",
       "9          9    TRAVEL  [warn, never, get, item, custom, u, custom, co...\n",
       "11        11    TRAVEL  [get, away, without, get, airplan, trip, easi,...\n",
       "...      ...       ...                                                ...\n",
       "22898  22898    TRAVEL  [berlin, cruis, ship, northern, europ, scandin...\n",
       "22910  22910  BUSINESS  [peac, corp, taught, manag, peopl, simultan, w...\n",
       "22914  22914  BUSINESS  [bank, america, appear, flip, firearm, promis,...\n",
       "22922  22922    TRAVEL  [golden, giveaway, travelzoo, global, internet...\n",
       "22923  22923    TRAVEL  [talk, actor, activist, innkeep, richard, gere...\n",
       "\n",
       "[13029 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframeTB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create train and test groups. Train set consists of 80% of data and test will contain the rest. Selection of these sets are random and dataset eas splitted to 2 groups before slicing it to be ure that each contain 80% and 20% of both classes. The reason for that is to train our model to classify both classes and it needs a normal data for that but randomness of our selection could cause problem by assigning 70% of business class and 90% of travel class and it could make our classifier biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = dataframeTB.loc[dataframeTB['category'] == 'BUSINESS']\n",
    "T = dataframeTB.loc[dataframeTB['category'] == 'TRAVEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "B['random_number'] = abs(np.random.random(len(B)))\n",
    "trainB = B[B['random_number'] <= 0.8]\n",
    "testB = B[B['random_number'] > 0.8]\n",
    "T['random_number'] = abs(np.random.random(len(T)))\n",
    "trainT = T[T['random_number'] <= 0.8]\n",
    "testT = T[T['random_number'] > 0.8]\n",
    "frames = [trainB, trainT]\n",
    "train = pd.concat(frames)\n",
    "frames = [testB, testT]\n",
    "test = copy.deepcopy(pd.concat(frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before any operation we should define our probabilities: Posterior Probability( P(c|x) ) is the probability of category of news being c if words of the text are x. Likelihood is reverse of this probability meaning that it is probability of texts containing words equal to x if news category is c. Class prior probability is the probability of a news category publishing across all of the news published which is given to us ny an expert of this field. In this case I used the numbers of thisdataset on kaggle which had more than 200k of news data. Last one is predictor prior probability which is probability of a text cinsisting of set of word equal to x in general which we don't calculate in this project because we want to valculate maximum posterior probability between classes and this element is constant in all of classes and wouldn't affect the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PcB = math.log(5937 / 200853, 10) #according to huffington post's dataset\n",
    "PcT = math.log(9887 / 200853, 10)\n",
    "PcS = math.log(9649 / 200853, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>random_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[break, bad, final, ad, cost, much, report, fa...</td>\n",
       "      <td>0.787668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[cleveland, citi, council, vote, rais, minimum...</td>\n",
       "      <td>0.011517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[huge, california, ga, leak, come, end, februa...</td>\n",
       "      <td>0.175417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[women, busi, q, mari, pearson, founder, pears...</td>\n",
       "      <td>0.377325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[wall, street, lose, confid, exxon, mobil, low...</td>\n",
       "      <td>0.545618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22894</th>\n",
       "      <td>22894</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[world, worst, airlin, name, airlin, name, tru...</td>\n",
       "      <td>0.430988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22897</th>\n",
       "      <td>22897</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[exit, row, seat, get, much, everyon, love, li...</td>\n",
       "      <td>0.488723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22898</th>\n",
       "      <td>22898</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[berlin, cruis, ship, northern, europ, scandin...</td>\n",
       "      <td>0.169522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22922</th>\n",
       "      <td>22922</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[golden, giveaway, travelzoo, global, internet...</td>\n",
       "      <td>0.408819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22923</th>\n",
       "      <td>22923</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[talk, actor, activist, innkeep, richard, gere...</td>\n",
       "      <td>0.498853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10505 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  category                                               text  \\\n",
       "15        15  BUSINESS  [break, bad, final, ad, cost, much, report, fa...   \n",
       "18        18  BUSINESS  [cleveland, citi, council, vote, rais, minimum...   \n",
       "23        23  BUSINESS  [huge, california, ga, leak, come, end, februa...   \n",
       "28        28  BUSINESS  [women, busi, q, mari, pearson, founder, pears...   \n",
       "43        43  BUSINESS  [wall, street, lose, confid, exxon, mobil, low...   \n",
       "...      ...       ...                                                ...   \n",
       "22894  22894    TRAVEL  [world, worst, airlin, name, airlin, name, tru...   \n",
       "22897  22897    TRAVEL  [exit, row, seat, get, much, everyon, love, li...   \n",
       "22898  22898    TRAVEL  [berlin, cruis, ship, northern, europ, scandin...   \n",
       "22922  22922    TRAVEL  [golden, giveaway, travelzoo, global, internet...   \n",
       "22923  22923    TRAVEL  [talk, actor, activist, innkeep, richard, gere...   \n",
       "\n",
       "       random_number  \n",
       "15          0.787668  \n",
       "18          0.011517  \n",
       "23          0.175417  \n",
       "28          0.377325  \n",
       "43          0.545618  \n",
       "...              ...  \n",
       "22894       0.430988  \n",
       "22897       0.488723  \n",
       "22898       0.169522  \n",
       "22922       0.408819  \n",
       "22923       0.498853  \n",
       "\n",
       "[10505 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>random_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[still, pretti, hard, women, get, free, birth,...</td>\n",
       "      <td>0.879885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[nation, mortgag, settlement, miss, final, dea...</td>\n",
       "      <td>0.890291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[state, spend, least, educ, wall, st, base, u,...</td>\n",
       "      <td>0.821718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[elizabeth, warren, take, uber, lyft, gig, eco...</td>\n",
       "      <td>0.958009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[truth, statement, nonprofit, actual, busi, ma...</td>\n",
       "      <td>0.928863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22836</th>\n",
       "      <td>22836</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[travel, loyalti, sale, mayb, time, rethink, m...</td>\n",
       "      <td>0.913720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22840</th>\n",
       "      <td>22840</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[state, street, ballet, perfect, way, spend, h...</td>\n",
       "      <td>0.999828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22859</th>\n",
       "      <td>22859</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[ryanair, announc, massiv, billion, order, boe...</td>\n",
       "      <td>0.970037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22861</th>\n",
       "      <td>22861</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[top, place, escap, thanksgiv, ever, urg, esca...</td>\n",
       "      <td>0.814097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22885</th>\n",
       "      <td>22885</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>[nyc, local, guid, first, thing, peopl, mentio...</td>\n",
       "      <td>0.933505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2524 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  category                                               text  \\\n",
       "4          4  BUSINESS  [still, pretti, hard, women, get, free, birth,...   \n",
       "26        26  BUSINESS  [nation, mortgag, settlement, miss, final, dea...   \n",
       "53        53  BUSINESS  [state, spend, least, educ, wall, st, base, u,...   \n",
       "63        63  BUSINESS  [elizabeth, warren, take, uber, lyft, gig, eco...   \n",
       "66        66  BUSINESS  [truth, statement, nonprofit, actual, busi, ma...   \n",
       "...      ...       ...                                                ...   \n",
       "22836  22836    TRAVEL  [travel, loyalti, sale, mayb, time, rethink, m...   \n",
       "22840  22840    TRAVEL  [state, street, ballet, perfect, way, spend, h...   \n",
       "22859  22859    TRAVEL  [ryanair, announc, massiv, billion, order, boe...   \n",
       "22861  22861    TRAVEL  [top, place, escap, thanksgiv, ever, urg, esca...   \n",
       "22885  22885    TRAVEL  [nyc, local, guid, first, thing, peopl, mentio...   \n",
       "\n",
       "       random_number  \n",
       "4           0.879885  \n",
       "26          0.890291  \n",
       "53          0.821718  \n",
       "63          0.958009  \n",
       "66          0.928863  \n",
       "...              ...  \n",
       "22836       0.913720  \n",
       "22840       0.999828  \n",
       "22859       0.970037  \n",
       "22861       0.814097  \n",
       "22885       0.933505  \n",
       "\n",
       "[2524 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWords(df):\n",
    "    words = []\n",
    "    for row in range(len(df)):\n",
    "        words += df.iloc[row]['text']\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to train the model by calculating probability of each word in each class by counting its frequecy in all of the texts with that class category. An important point here is to use and store probabilities using logarithm because they are such a small numbers that we cant store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBWords = extractWords(trainB)\n",
    "n = len(set(trainBWords))\n",
    "BProbabilities = dict( (l, math.log(trainBWords.count(l) / n, 10)) for l in set(trainBWords))\n",
    "trainBWords = set(trainBWords)\n",
    "trainTWords = extractWords(trainT)\n",
    "m = len(set(trainTWords))\n",
    "TProbabilities = dict( (l, math.log(trainTWords.count(l) / m, 10)) for l in set(trainTWords))\n",
    "trainTWords = set(trainTWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is where we use probability of each word to classify the test data group. An important point here is to give a very low probability to a word when it is not available in trained word of a class vecause ignoring it is as same as giving it probability of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for row in range(len(test)):\n",
    "    PBrow = PcB\n",
    "    PTrow = PcT\n",
    "    for word in test.iloc[row]['text']:\n",
    "        if word in trainBWords and word in trainTWords:\n",
    "            PBrow += BProbabilities[word]\n",
    "            PTrow += TProbabilities[word]\n",
    "        if word in trainBWords and not word in trainTWords:\n",
    "            PBrow += BProbabilities[word]\n",
    "            PTrow += -4\n",
    "        if not word in trainBWords and word in trainTWords:\n",
    "            PTrow += TProbabilities[word]\n",
    "            PBrow += -4      \n",
    "    if PBrow >= PTrow:\n",
    "        result.append('BUSINESS')\n",
    "    else:\n",
    "        result.append('TRAVEL')\n",
    "test['classified'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall of TRAVEL : 97.02233250620348     Recall of BUSINESS : 87.82894736842105\n",
      "Precision of TRAVEL : 93.3731343283582   Precision of BUSINESS : 94.34628975265018\n",
      "Accuracy : 93.70047543581617\n"
     ]
    }
   ],
   "source": [
    "numOfCorrectClassified = 0\n",
    "correctClassifiedB = 0\n",
    "correctClassifiedT = 0\n",
    "detectedB = 0\n",
    "detectedT = 0\n",
    "for i in range(len(test)):\n",
    "    if test.iloc[i]['classified'] == 'TRAVEL':\n",
    "        detectedT += 1\n",
    "    else:\n",
    "        detectedB += 1\n",
    "    if test.iloc[i]['classified'] == test.iloc[i]['category']:\n",
    "        numOfCorrectClassified += 1\n",
    "        if test.iloc[i]['classified'] == 'TRAVEL':\n",
    "            correctClassifiedT += 1\n",
    "        else:\n",
    "            correctClassifiedB += 1\n",
    "\n",
    "print('Recall of TRAVEL :', (correctClassifiedT / len(testT)) * 100, '    Recall of BUSINESS :', (correctClassifiedB / len(testB)) * 100)\n",
    "print('Precision of TRAVEL :', (correctClassifiedT / detectedT) * 100, '  Precision of BUSINESS :', (correctClassifiedB / detectedB) * 100)\n",
    "print('Accuracy :', (numOfCorrectClassified / len(test)) * 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all of evaluation parameters are higher than minimum accuracy needed. Now we implement steps above an aclassifier with 3 classes to classify instead of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = dataframe.loc[dataframe['category'] == 'BUSINESS']\n",
    "T = dataframe.loc[dataframe['category'] == 'TRAVEL']\n",
    "S = dataframe.loc[dataframe['category'] == 'STYLE & BEAUTY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "B['random_number'] = abs(np.random.random(len(B)))\n",
    "trainB = B[B['random_number'] <= 0.8]\n",
    "testB = B[B['random_number'] > 0.8]\n",
    "T['random_number'] = abs(np.random.random(len(T)))\n",
    "trainT = T[T['random_number'] <= 0.8]\n",
    "testT = T[T['random_number'] > 0.8]\n",
    "S['random_number'] = abs(np.random.random(len(S)))\n",
    "trainS = S[S['random_number'] <= 0.8]\n",
    "testS = S[S['random_number'] > 0.8]\n",
    "frames = [trainB, trainT, trainS]\n",
    "train = pd.concat(frames)\n",
    "frames = [testB, testT, testS]\n",
    "test = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBWords = extractWords(trainB)\n",
    "n = len(set(trainBWords))\n",
    "BProbabilities = dict( (l, math.log(trainBWords.count(l) / n, 10)) for l in set(trainBWords))\n",
    "trainBWords = set(trainBWords)\n",
    "trainTWords = extractWords(trainT)\n",
    "m = len(set(trainTWords))\n",
    "TProbabilities = dict( (l, math.log(trainTWords.count(l) / m, 10)) for l in set(trainTWords))\n",
    "trainTWords = set(trainTWords)\n",
    "trainSWords = extractWords(trainS)\n",
    "k = len(set(trainSWords))\n",
    "SProbabilities = dict( (l, math.log(trainSWords.count(l) / k, 10)) for l in set(trainSWords))\n",
    "trainSWords = set(trainSWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for row in range(len(test)):\n",
    "    PBrow = PcB\n",
    "    PTrow = PcT\n",
    "    PSrow = PcS\n",
    "    for word in test.iloc[row]['text']:\n",
    "        if word in trainBWords:\n",
    "            PBrow += BProbabilities[word]\n",
    "        else:\n",
    "            PBrow += -4\n",
    "        if word in trainTWords:\n",
    "            PTrow += TProbabilities[word]\n",
    "        else:\n",
    "            PTrow += -4\n",
    "        if word in trainSWords:\n",
    "            PSrow += SProbabilities[word]\n",
    "        else:\n",
    "            PSrow += -4\n",
    "    if PBrow >= PTrow and PBrow >= PSrow:\n",
    "        result.append('BUSINESS')\n",
    "    elif PTrow >= PBrow and PTrow >= PSrow:\n",
    "        result.append('TRAVEL')\n",
    "    else:\n",
    "        result.append('STYLE & BEAUTY')\n",
    "test['classified'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>random_number</th>\n",
       "      <th>classified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[cleveland, citi, council, vote, rais, minimum...</td>\n",
       "      <td>0.900820</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[go, nowher, fast, mcdonald, forget, salad, wr...</td>\n",
       "      <td>0.837008</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[trendspot, still, matter, power, look, forwar...</td>\n",
       "      <td>0.920603</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[nike, middl, east, badass, new, ad, break, ba...</td>\n",
       "      <td>0.945046</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>[vulner, busi, good, thing, us, fundament, bel...</td>\n",
       "      <td>0.994631</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22904</th>\n",
       "      <td>22904</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>[top, ten, right, list, make, happi, good, old...</td>\n",
       "      <td>0.813030</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22907</th>\n",
       "      <td>22907</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>[dear, mari, clair, magazin, instead, worri, l...</td>\n",
       "      <td>0.816937</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22916</th>\n",
       "      <td>22916</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>[new, season, new, makeup, trend, spring, desp...</td>\n",
       "      <td>0.959821</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22920</th>\n",
       "      <td>22920</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>[rihanna, sweatpant, top, worst, dress, list, ...</td>\n",
       "      <td>0.910603</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22921</th>\n",
       "      <td>22921</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>[gisel, give, birth, babi, girl, model, welcom...</td>\n",
       "      <td>0.903447</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4386 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index        category  \\\n",
       "18        18        BUSINESS   \n",
       "75        75        BUSINESS   \n",
       "77        77        BUSINESS   \n",
       "82        82        BUSINESS   \n",
       "87        87        BUSINESS   \n",
       "...      ...             ...   \n",
       "22904  22904  STYLE & BEAUTY   \n",
       "22907  22907  STYLE & BEAUTY   \n",
       "22916  22916  STYLE & BEAUTY   \n",
       "22920  22920  STYLE & BEAUTY   \n",
       "22921  22921  STYLE & BEAUTY   \n",
       "\n",
       "                                                    text  random_number  \\\n",
       "18     [cleveland, citi, council, vote, rais, minimum...       0.900820   \n",
       "75     [go, nowher, fast, mcdonald, forget, salad, wr...       0.837008   \n",
       "77     [trendspot, still, matter, power, look, forwar...       0.920603   \n",
       "82     [nike, middl, east, badass, new, ad, break, ba...       0.945046   \n",
       "87     [vulner, busi, good, thing, us, fundament, bel...       0.994631   \n",
       "...                                                  ...            ...   \n",
       "22904  [top, ten, right, list, make, happi, good, old...       0.813030   \n",
       "22907  [dear, mari, clair, magazin, instead, worri, l...       0.816937   \n",
       "22916  [new, season, new, makeup, trend, spring, desp...       0.959821   \n",
       "22920  [rihanna, sweatpant, top, worst, dress, list, ...       0.910603   \n",
       "22921  [gisel, give, birth, babi, girl, model, welcom...       0.903447   \n",
       "\n",
       "           classified  \n",
       "18           BUSINESS  \n",
       "75           BUSINESS  \n",
       "77           BUSINESS  \n",
       "82     STYLE & BEAUTY  \n",
       "87           BUSINESS  \n",
       "...               ...  \n",
       "22904  STYLE & BEAUTY  \n",
       "22907  STYLE & BEAUTY  \n",
       "22916  STYLE & BEAUTY  \n",
       "22920  STYLE & BEAUTY  \n",
       "22921  STYLE & BEAUTY  \n",
       "\n",
       "[4386 rows x 5 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall of TRAVEL : 94.03697078115682     Recall of BUSINESS : 84.65553235908142     Recall of STYLE & BEAUTY : 96.0022844089092\n",
      "Precision of TRAVEL : 92.49266862170089   Precision of BUSINESS : 92.68571428571428   Precision of STYLE & BEAUTY : 93.07862679955703\n",
      "Accuracy : 92.77245782033744\n"
     ]
    }
   ],
   "source": [
    "numOfCorrectClassified = 0\n",
    "correctClassifiedB = 0\n",
    "correctClassifiedT = 0\n",
    "correctClassifiedS = 0\n",
    "detectedB = 0\n",
    "detectedT = 0\n",
    "detectedS = 0\n",
    "PBAT = 0\n",
    "PSAT = 0\n",
    "PTAB = 0\n",
    "PSAB = 0\n",
    "PTAS = 0\n",
    "PBAS = 0\n",
    "for i in range(len(test)):\n",
    "    if test.iloc[i]['classified'] == 'BUSINESS' and test.iloc[i]['category'] == \"TRAVEL\":\n",
    "        PBAT += 1\n",
    "    if test.iloc[i]['classified'] == 'STYLE & BEAUTY' and test.iloc[i]['category'] == \"TRAVEL\":\n",
    "        PSAT += 1\n",
    "    if test.iloc[i]['classified'] == 'TRAVEL' and test.iloc[i]['category'] == \"BUSINESS\":\n",
    "        PTAB += 1\n",
    "    if test.iloc[i]['classified'] == 'STYLE & BEAUTY' and test.iloc[i]['category'] == \"BUSINESS\":\n",
    "        PSAB += 1\n",
    "    if test.iloc[i]['classified'] == 'TRAVEL' and test.iloc[i]['category'] == \"STYLE & BEAUTY\":\n",
    "        PTAS += 1\n",
    "    if test.iloc[i]['classified'] == 'BUSINESS' and test.iloc[i]['category'] == \"STYLE & BEAUTY\":\n",
    "        PBAS += 1\n",
    "        \n",
    "    if test.iloc[i]['classified'] == 'TRAVEL':\n",
    "        detectedT += 1\n",
    "    elif test.iloc[i]['classified'] == 'BUSINESS':\n",
    "        detectedB += 1\n",
    "    else:\n",
    "        detectedS += 1\n",
    "    if test.iloc[i]['classified'] == test.iloc[i]['category']:\n",
    "        numOfCorrectClassified += 1\n",
    "        if test.iloc[i]['classified'] == 'TRAVEL':\n",
    "            correctClassifiedT += 1\n",
    "        elif test.iloc[i]['classified'] == 'BUSINESS':\n",
    "            correctClassifiedB += 1\n",
    "        else:\n",
    "            correctClassifiedS += 1\n",
    "print('Recall of TRAVEL :', (correctClassifiedT / len(testT)) * 100, '    Recall of BUSINESS :',\n",
    "      (correctClassifiedB / len(testB)) * 100, '    Recall of STYLE & BEAUTY :', (correctClassifiedS / len(testS)) * 100)\n",
    "print('Precision of TRAVEL :', (correctClassifiedT / detectedT) * 100, '  Precision of BUSINESS :',\n",
    "      (correctClassifiedB / detectedB) * 100,  '  Precision of STYLE & BEAUTY :', (correctClassifiedS / detectedS) * 100)\n",
    "print('Accuracy :', (numOfCorrectClassified / len(test)) * 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A confusion matrix is a table that is often used to describe the performance of a classifier on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm. Most performance measures are computed from the confusion matrix. The confusion matrix shows the ways in which your classification model is confused when it makes predictions. It gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made. The first column of this table shows number of data that classifier classified as travel and each row of this column is actual number of data classified as travel, by each category. It means that the left top of table shows number of data that was actualy travel and classifier predicted it Travel. It is better for us to have higher numbers in diameter and lower in other cells. It could be seen that there are many data that are predicted style and beauty falsely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   n = 4386        Predicted Travel    Predicted Business  Predicted Style & B\n",
      "Actual Travel            1577                 36                  64\n",
      "Actual Business           86                  811                 61\n",
      "Actual Style & B          42                  28                  1681\n"
     ]
    }
   ],
   "source": [
    "print(\"   n =\", len(test), \"       Predicted Travel    Predicted Business  Predicted Style & B\")\n",
    "print(\"Actual Travel           \", correctClassifiedT, \"               \", PBAT, \"                \", PSAT)\n",
    "print(\"Actual Business          \", PTAB, \"                \", correctClassifiedB, \"               \", PSAB)         \n",
    "print(\"Actual Style & B         \", PTAS, \"                \", PBAS, \"                \", correctClassifiedS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could be seen that precision and recall of business class have a noticable difference and its reason is that number of business classes for training are lower than other classes so our word dictionary for this class is weaker and if a new word appear in our test it is more possible that it is available in the better-trained classes. Another thing is that when we add random data from current dataset data with higher probability is more probable to be chosen to add so its probability will increase and data with lower probability will be less probable. There are some ways to handle this like oversampling, undersampling, SMOTE and ... . The method we use here is oversampling in which we choose random rows from train data and add them to the train so that its population gets closer to other classes. This action will increase probability of each word meaning that weaker classes will be chosen more often. Adding about a 1000 rows almost equalize this evaluation metrics for all classes. Accuracy and recall don't change much but precision for business class gets better. An important point in oversapling is that train and test should be splitted before this becuse if you add from test to train it will discredit the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STYLE & BEAUTY    8674\n",
       "TRAVEL            8461\n",
       "BUSINESS          4568\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = dataframe['category'].value_counts()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "D:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "B = dataframe.loc[dataframe['category'] == 'BUSINESS']\n",
    "difference = int(((counts[0] + counts[1]) / 2) - counts[2]) - 3000\n",
    "n = len(B)\n",
    "t = len(B)\n",
    "for c in range(difference):\n",
    "    randomNumber = random.randint(0, n - 1)\n",
    "    B.loc[t] = [B.iloc[randomNumber].iloc[0], B.iloc[randomNumber].iloc[1], B.iloc[randomNumber].iloc[2]]\n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall of TRAVEL : 91.47869674185463     Recall of BUSINESS : 90.83409715857012     Recall of STYLE & BEAUTY : 94.53357100415924\n",
      "Precision of TRAVEL : 93.89067524115757   Precision of BUSINESS : 88.87892376681614   Precision of STYLE & BEAUTY : 93.58823529411765\n",
      "Accuracy : 92.49427917620137\n",
      "   n = 4370        Predicted Travel    Predicted Business  Predicted Style & B\n",
      "Actual Travel            1460                 74                  62\n",
      "Actual Business           53                  991                 47\n",
      "Actual Style & B          42                  50                  1591\n"
     ]
    }
   ],
   "source": [
    "B['random_number'] = abs(np.random.random(len(B)))\n",
    "trainB = B[B['random_number'] <= 0.8]\n",
    "testB = B[B['random_number'] > 0.8]\n",
    "T['random_number'] = abs(np.random.random(len(T)))\n",
    "trainT = T[T['random_number'] <= 0.8]\n",
    "testT = T[T['random_number'] > 0.8]\n",
    "S['random_number'] = abs(np.random.random(len(S)))\n",
    "trainS = S[S['random_number'] <= 0.8]\n",
    "testS = S[S['random_number'] > 0.8]\n",
    "frames = [trainB, trainT, trainS]\n",
    "train = pd.concat(frames)\n",
    "frames = [testB, testT, testS]\n",
    "test = pd.concat(frames)\n",
    "trainBWords = extractWords(trainB)\n",
    "n = len(set(trainBWords))\n",
    "BProbabilities = dict( (l, math.log(trainBWords.count(l) / n, 10)) for l in set(trainBWords))\n",
    "trainBWords = set(trainBWords)\n",
    "trainTWords = extractWords(trainT)\n",
    "m = len(set(trainTWords))\n",
    "TProbabilities = dict( (l, math.log(trainTWords.count(l) / m, 10)) for l in set(trainTWords))\n",
    "trainTWords = set(trainTWords)\n",
    "trainSWords = extractWords(trainS)\n",
    "m = len(set(trainSWords))\n",
    "SProbabilities = dict( (l, math.log(trainSWords.count(l) / m, 10)) for l in set(trainSWords))\n",
    "trainSWords = set(trainSWords)\n",
    "result = []\n",
    "for row in range(len(test)):\n",
    "    PBrow = PcB\n",
    "    PTrow = PcT\n",
    "    PSrow = PcS\n",
    "    for word in test.iloc[row]['text']:\n",
    "        if word in trainBWords:\n",
    "            PBrow += BProbabilities[word]\n",
    "        else:\n",
    "            PBrow += -4\n",
    "        if word in trainTWords:\n",
    "            PTrow += TProbabilities[word]\n",
    "        else:\n",
    "            PTrow += -4\n",
    "        if word in trainSWords:\n",
    "            PSrow += SProbabilities[word]\n",
    "        else:\n",
    "            PSrow += -4\n",
    "    if PBrow >= PTrow and PBrow >= PSrow:\n",
    "        result.append('BUSINESS')\n",
    "    elif PTrow >= PBrow and PTrow >= PSrow:\n",
    "        result.append('TRAVEL')\n",
    "    else:\n",
    "        result.append('STYLE & BEAUTY')\n",
    "test['classified'] = result\n",
    "numOfCorrectClassified = 0\n",
    "correctClassifiedB = 0\n",
    "correctClassifiedT = 0\n",
    "correctClassifiedS = 0\n",
    "detectedB = 0\n",
    "detectedT = 0\n",
    "detectedS = 0\n",
    "PBAT = 0\n",
    "PSAT = 0\n",
    "PTAB = 0\n",
    "PSAB = 0\n",
    "PTAS = 0\n",
    "PBAS = 0\n",
    "for i in range(len(test)):\n",
    "    if test.iloc[i]['classified'] == 'BUSINESS' and test.iloc[i]['category'] == \"TRAVEL\":\n",
    "        PBAT += 1\n",
    "    if test.iloc[i]['classified'] == 'STYLE & BEAUTY' and test.iloc[i]['category'] == \"TRAVEL\":\n",
    "        PSAT += 1\n",
    "    if test.iloc[i]['classified'] == 'TRAVEL' and test.iloc[i]['category'] == \"BUSINESS\":\n",
    "        PTAB += 1\n",
    "    if test.iloc[i]['classified'] == 'STYLE & BEAUTY' and test.iloc[i]['category'] == \"BUSINESS\":\n",
    "        PSAB += 1\n",
    "    if test.iloc[i]['classified'] == 'TRAVEL' and test.iloc[i]['category'] == \"STYLE & BEAUTY\":\n",
    "        PTAS += 1\n",
    "    if test.iloc[i]['classified'] == 'BUSINESS' and test.iloc[i]['category'] == \"STYLE & BEAUTY\":\n",
    "        PBAS += 1\n",
    "        \n",
    "    if test.iloc[i]['classified'] == 'TRAVEL':\n",
    "        detectedT += 1\n",
    "    elif test.iloc[i]['classified'] == 'BUSINESS':\n",
    "        detectedB += 1\n",
    "    else:\n",
    "        detectedS += 1\n",
    "    if test.iloc[i]['classified'] == test.iloc[i]['category']:\n",
    "        numOfCorrectClassified += 1\n",
    "        if test.iloc[i]['classified'] == 'TRAVEL':\n",
    "            correctClassifiedT += 1\n",
    "        elif test.iloc[i]['classified'] == 'BUSINESS':\n",
    "            correctClassifiedB += 1\n",
    "        else:\n",
    "            correctClassifiedS += 1\n",
    "print('Recall of TRAVEL :', (correctClassifiedT / len(testT)) * 100, '    Recall of BUSINESS :',\n",
    "      (correctClassifiedB / len(testB)) * 100, '    Recall of STYLE & BEAUTY :', (correctClassifiedS / len(testS)) * 100)\n",
    "print('Precision of TRAVEL :', (correctClassifiedT / detectedT) * 100, '  Precision of BUSINESS :',\n",
    "      (correctClassifiedB / detectedB) * 100,  '  Precision of STYLE & BEAUTY :', (correctClassifiedS / detectedS) * 100)\n",
    "print('Accuracy :', (numOfCorrectClassified / len(test)) * 100 )\n",
    "print(\"   n =\", len(test), \"       Predicted Travel    Predicted Business  Predicted Style & B\")\n",
    "print(\"Actual Travel           \", correctClassifiedT, \"               \", PBAT, \"                \", PSAT)\n",
    "print(\"Actual Business          \", PTAB, \"                \", correctClassifiedB, \"               \", PSAB)         \n",
    "print(\"Actual Style & B         \", PTAS, \"                \", PBAS, \"                \", correctClassifiedS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5363 8461 8674\n"
     ]
    }
   ],
   "source": [
    "print(len(B), len(T), len(S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to classify unknown data give to us in test.csv file. After loading the dataset we clean data and split texts to words and classify news based on their words and model trained in last part. I checked the result written in output.csv with the correct data from kaggle.com and got 95% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "unclassified = pd.read_csv('test.csv')\n",
    "stopWords = set(stopwords.words('english'))\n",
    "def cleanData(row):\n",
    "    if row['short_description'] == None:\n",
    "        row['text'] = row['headline'] + ' ' + row['short_description']\n",
    "    else:\n",
    "        row['text'] = row['headline']\n",
    "    row['text'] = row['text'].lower()\n",
    "    wordTokens = re.split('\\W', row['text'])\n",
    "    filtered = [] \n",
    "    ps = PorterStemmer()\n",
    "    for word in wordTokens:\n",
    "        if word not in stopWords and not word == '' and not word.isdigit():\n",
    "            root = ps.stem(word)\n",
    "            filtered.append(root)\n",
    "    row['text'] = filtered\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[kate, middleton, one, two, style, win, new, z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[instagram, local, len, seri, featur, insid, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[go, thanksgiv]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[retail, hire, employe, holiday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[get, flat, iron, wave, minut, video]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2543</th>\n",
       "      <td>2543</td>\n",
       "      <td>[state, line, fiscal, cliff, debat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544</th>\n",
       "      <td>2544</td>\n",
       "      <td>[multipurpos, travel, essenti, need, next, adv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545</th>\n",
       "      <td>2545</td>\n",
       "      <td>[santa, claus, unemploy, holiday, season, video]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2546</th>\n",
       "      <td>2546</td>\n",
       "      <td>[like, food, tv, train, women, leader, culinar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>2547</td>\n",
       "      <td>[lilli, pulitz, dead, fashion, design, die]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2548 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               text\n",
       "0         0  [kate, middleton, one, two, style, win, new, z...\n",
       "1         1  [instagram, local, len, seri, featur, insid, l...\n",
       "2         2                                    [go, thanksgiv]\n",
       "3         3                   [retail, hire, employe, holiday]\n",
       "4         4              [get, flat, iron, wave, minut, video]\n",
       "...     ...                                                ...\n",
       "2543   2543                [state, line, fiscal, cliff, debat]\n",
       "2544   2544  [multipurpos, travel, essenti, need, next, adv...\n",
       "2545   2545   [santa, claus, unemploy, holiday, season, video]\n",
       "2546   2546  [like, food, tv, train, women, leader, culinar...\n",
       "2547   2547        [lilli, pulitz, dead, fashion, design, die]\n",
       "\n",
       "[2548 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unclassified = unclassified.apply(cleanData, axis = 'columns')\n",
    "unclassified = unclassified.loc[:, ['index', 'text']]\n",
    "unclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for row in range(len(unclassified)):\n",
    "    PBrow = PcB\n",
    "    PTrow = PcT\n",
    "    PSrow = PcS\n",
    "    for word in unclassified.iloc[row]['text']:\n",
    "\n",
    "        if word in trainBWords:\n",
    "            PBrow += BProbabilities[word]\n",
    "        else:\n",
    "            PBrow += -4\n",
    "        if word in trainTWords:\n",
    "            PTrow += TProbabilities[word]\n",
    "        else:\n",
    "            PTrow += -4\n",
    "        if word in trainSWords:\n",
    "            PSrow += SProbabilities[word]\n",
    "        else:\n",
    "            PSrow += -4\n",
    "    if PBrow >= PTrow and PBrow >= PSrow:\n",
    "        result.append('BUSINESS')\n",
    "    elif PTrow >= PBrow and PTrow >= PSrow:\n",
    "        result.append('TRAVEL')\n",
    "    else:\n",
    "        result.append('STYLE & BEAUTY')\n",
    "unclassified['classified'] = result\n",
    "unclassified = unclassified.loc[:, ['index', 'classified']]\n",
    "unclassified.to_csv('output.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "#### 1- \n",
    "I answered the first question in the report. Although lemmatization is more advaned they both result in a same way.\n",
    "#### 2- \n",
    "The tf-idf weight a statistical measure used to evaluate how important a word is to a document. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the dataset. This weight is computed by finding product of term frequency (tf) and inverse document frequency. Term Frequency indicates the number of occurences of a particular term t in document d. We can see that as a term appears more in the document it becomes more important, which is logical. We need to normalize it by by dividing it by total number of words. The idf of a term is the number of documents in the dataset divided by the document frequency of a term. Term frequency is the occurrence count of a term in one particular document only; while document frequency is the number of different documents the term appears in, so it depends on the whole dataset. It’s expected that the more frequent term to be considered less important, but the factor (most probably integers) seems too harsh. Therefore, we take the logarithm (with base 2 ) of the inverse document frequencies. The last step is to calculate product of these two measures for each word. In order to use this measure we could have used it instead of likelihood that we calculated in previous steps.\n",
    "#### 3-\n",
    "Prescision is an important evaluation measure but it is not enough and focusing on just this measure will cause problem; Our model could be riskless and classify only items that is sure about them and this will decrease accuracy and recall. For example if we have a classifier for ham and spam detection and we choose only one email as spam that has the highest probability we will probably get precision of 100% but accuracy and recall will be very low. If precision for all classes of classifier are high then our classifier has worked realy well because we have classified all of data and did it flawless so accuracy and recall will also be high.\n",
    "#### 4-\n",
    "If a word appears in train in one class only, in the step that we want calculate probability of each class for test data the class containing this word will get a high probability and other classes will get a small number( 10^-4) that will make them less probable and data will probably be classified as category containing spoken word."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
